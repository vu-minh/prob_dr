{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Probability version:  1.10.0-dev20180716\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "import warnings\n",
    "\n",
    "from observations import iris, mnist\n",
    "DATA_DIR = './data'\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Tensorflow Probability version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (60000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist(DATA_DIR)\n",
    "x_train = x_train.T\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, metadata = iris(DATA_DIR)\n",
    "\n",
    "# standardize data\n",
    "# x_mean = np.mean(x_train, axis=0)\n",
    "# x_std = np.std(x_train, axis=0)\n",
    "# x_train = (x_train - x_mean) / x_std\n",
    "\n",
    "x_train = x_train.T\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(metadata)\n",
    "\n",
    "y_label = np.unique(y_train)\n",
    "y_dict = dict((label, i) for i, label in enumerate(y_label))\n",
    "y_class = list(map(lambda label: y_dict[label], y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Marginal distribution of each data point:**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{x}_n \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{W} \\mathbf{W}^T + \\sigma^2 \\mathbf{I})\n",
    "\\end{equation*}\n",
    "\n",
    "+ Create a model as follow:\n",
    "    * $\\mathbf{z}_n \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "    * $\\mathbf{W} \\sim \\mathcal{N}(\\mathbf{0}, 2 \\mathbf{I})$\n",
    "    * $\\mathbf{x}_n | \\mathbf{z}_n \\sim \\mathcal{N}(\n",
    "        \\mathbf{W} \\mathbf{z}_n, \\sigma^2 \\mathbf{T} \n",
    "      )$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppca_model(data_dim, latent_dim, num_datapoints, stddev_datapoints):\n",
    "    w = ed.Normal(loc=tf.zeros([data_dim, latent_dim]),\n",
    "                  scale=tf.ones([data_dim, latent_dim]),\n",
    "                  name=\"w\") # parameter\n",
    "    z = ed.Normal(loc=tf.zeros([latent_dim, num_datapoints]),\n",
    "                  scale=tf.ones([latent_dim, num_datapoints]),\n",
    "                  name=\"z\") # parameter\n",
    "    x = ed.Normal(loc=tf.matmul(w, z),\n",
    "                  scale=stddev_datapoints * tf.ones([data_dim, num_datapoints]),\n",
    "                  name=\"x\") # modeled (observed) data\n",
    "    return x, (w, z)\n",
    "\n",
    "log_joint = ed.make_log_joint_fn(ppca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 2\n"
     ]
    }
   ],
   "source": [
    "num_datapoints = x_train.shape[1]\n",
    "data_dim = x_train.shape[0]\n",
    "latent_dim = 2\n",
    "stddev_datapoints = 0.5\n",
    "\n",
    "# model = ppca_model(data_dim=data_dim,\n",
    "#                    latent_dim=latent_dim,\n",
    "#                    num_datapoints=num_datapoints,\n",
    "#                    stddev_datapoints=stddev_datapoints)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     x_train, (actual_w, actual_z) = sess.run(model)\n",
    "    \n",
    "# print(\"Principal axes: \")\n",
    "# print(actual_w)\n",
    "print(num_datapoints, data_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[0,:], x_train[1,:], color='blue', alpha=0.1)\n",
    "plt.axis([-10, 10, -10, 10])\n",
    "plt.title(\"Original Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP: Maximum A Posterior Inference\n",
    "\n",
    "+ **Goal**: find the point estimate of laten variables that maximizes the posterior probability density (maximum a posterior inference).\n",
    "+ **Note**: The point-estimate methods find the exact values for each latent var. The variational methods find the probability distribution of the value of each latent var.\n",
    "+ **How**: Calculating the values of $\\mathbf{W}$ and $\\mathbf{Z}$ that maxmize the posterior density: $p(\\mathbf{W}, \\mathbf{Z} | \\mathbf{X})\\propto p(\\mathbf{W}, \\mathbf{Z}, \\mathbf{X})$.\n",
    "Note that the `log_joint` function created by `ed.make_log_joint_fn` will represent for the log of the joint distribution of all variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we actually want to find is `w` and `z`\n",
    "# we have had their asumption \"prior\" values.\n",
    "# we need to use the observed data `x` to do inference:\n",
    "# that means, to transfer the prior to the posterior\n",
    "# and the final expected results will be in `w_inferred_map` and `z_inferred_map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define `w` and `z` as variable in the model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "w = tf.Variable(np.ones([data_dim, latent_dim]), dtype=tf.float32)\n",
    "z = tf.Variable(np.ones([latent_dim, num_datapoints]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, define a target function, which is an \"unnormalized target density\".\n",
    "# as a function of the parameters `w`, `z`\n",
    "# Note that we want to maximize the posterior distr.\n",
    "# which is proportional to the joint distribution of the model\n",
    "# -> the target function to maximize is exactly the joint distr. function\n",
    "def target(w, z):\n",
    "    return log_joint(data_dim=data_dim,\n",
    "              latent_dim=latent_dim,\n",
    "              num_datapoints=num_datapoints,\n",
    "              stddev_datapoints=stddev_datapoints,\n",
    "              w=w, z=z, x=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third, define the \"energy function\", which is the opposite of target function\n",
    "# we will use the off-the-shelf optimization module which minimize an objective function\n",
    "# that's why we conver the maximization prob. to mizimization one\n",
    "energy = - target(w, z)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.05)\n",
    "train = optimizer.minimize(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-19aacdf71795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# start to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmvu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Learning params === Infering\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "t = []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # init all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # start to train\n",
    "    for i in range(num_epochs):\n",
    "        sess.run(train)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            # get snapshot of the current values of function and vars\n",
    "            cE, cw, cz = sess.run([energy, w, z])\n",
    "            t.append(cE)\n",
    "    \n",
    "    # when finishing the training, get the learned values of the variables\n",
    "    w_inferred_map = sess.run(w)\n",
    "    z_inferred_map = sess.run(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, num_epochs, 10)\n",
    "plt.plot(x, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Principal axes: \")\n",
    "# print(actual_w)\n",
    "\n",
    "print(\"MAP estimated axes:\")\n",
    "print(w_inferred_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model criticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Edward2 model to sample data from inferred values of W and Z\n",
    "\n",
    "def replace_latents(w=actual_w, z=actual_z):\n",
    "    def interceptor(rv_constructor, *rv_args, **rv_kwargs):\n",
    "        \"\"\"Replace the prior with actual values to generate samples from.\"\"\"\n",
    "        name = rv_kwargs.pop(\"name\")\n",
    "        if name == \"w\":\n",
    "            rv_kwargs[\"value\"] = w\n",
    "        elif name == \"z\":\n",
    "            rv_kwargs[\"value\"] = z\n",
    "        return rv_constructor(*rv_args, **rv_kwargs)\n",
    "    \n",
    "    return interceptor\n",
    "\n",
    "\n",
    "with ed.interception(replace_latents(w_inferred_map, z_inferred_map)):\n",
    "    generative_process = ppca_model(\n",
    "        data_dim=data_dim, latent_dim=latent_dim,\n",
    "        num_datapoints=num_datapoints, stddev_datapoints=stddev_datapoints\n",
    "    )\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    x_generated, _ = sess.run(generative_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the actual data with the simulated data \n",
    "# which is generated from the joint model with the MAP estimated `w` and `z`\n",
    "\n",
    "plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1, label='Actual data')\n",
    "plt.scatter(x_generated[0, :], x_generated[1, :], color='red', alpha=0.1, label='Simulated MAP')\n",
    "plt.legend()\n",
    "# plt.axis([-20, 20, -20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.05, label='Actual data')\n",
    "# plt.plot([-w_inferred_map[0][0], w_inferred_map[0][0]], [-w_inferred_map[1][0], w_inferred_map[1][0]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_inferred_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_inferred_map[0, :], z_inferred_map[1, :], c=y_class)\n",
    "for i in range(data_dim):\n",
    "    vx, vy = w_inferred_map[i] / np.linalg.norm(w_inferred_map[i])\n",
    "    print(vx, vy)\n",
    "    plt.plot([0,vx], [0,vy])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
